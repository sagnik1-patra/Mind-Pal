{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4132fa24-688b-458c-a6d2-150a1249ddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence to predict mood (or leave blank to use a CSV):  i am not very happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVE] Predictions -> C:\\Users\\sagni\\Downloads\\Mind Pal\\predictions_mood.csv\n"
     ]
    }
   ],
   "source": [
    "# === predict_mood.py (Jupyter-safe with interactive fallback) ===\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# -----------------------------\n",
    "# Default artifact locations\n",
    "# -----------------------------\n",
    "OUT_DIR   = r\"C:\\Users\\sagni\\Downloads\\Mind Pal\"\n",
    "PKL_PATH  = os.path.join(OUT_DIR, \"mindpal_preprocess.pkl\")\n",
    "H5_PATH   = os.path.join(OUT_DIR, \"mindpal_model.h5\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helper classes (must match training)\n",
    "# -----------------------------\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column): self.column = column\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X): return X[[self.column]]\n",
    "\n",
    "class To1DString(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X.iloc[:, 0].astype(str).values\n",
    "        return np.asarray(X).astype(str).ravel()\n",
    "\n",
    "class DateTimeExpand(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns): self.columns = columns; self.out_cols = []\n",
    "    def fit(self, X, y=None):\n",
    "        self.out_cols = []\n",
    "        for c in self.columns:\n",
    "            self.out_cols += [f\"{c}_year\", f\"{c}_month\", f\"{c}_day\", f\"{c}_dow\"]\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        outs = []\n",
    "        for c in self.columns:\n",
    "            s = pd.to_datetime(X[c], errors=\"coerce\")\n",
    "            outs.append(pd.DataFrame({\n",
    "                f\"{c}_year\":  s.dt.year.fillna(0).astype(int),\n",
    "                f\"{c}_month\": s.dt.month.fillna(0).astype(int),\n",
    "                f\"{c}_day\":   s.dt.day.fillna(0).astype(int),\n",
    "                f\"{c}_dow\":   s.dt.dayofweek.fillna(0).astype(int),\n",
    "            }))\n",
    "        return pd.concat(outs, axis=1) if outs else np.empty((len(X), 0))\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def load_bundle_and_model(pkl_path: str, h5_path: str):\n",
    "    if not os.path.exists(pkl_path):\n",
    "        raise FileNotFoundError(f\"Missing preprocess bundle: {pkl_path}\")\n",
    "    if not os.path.exists(h5_path):\n",
    "        raise FileNotFoundError(f\"Missing model file: {h5_path}\")\n",
    "    bundle = joblib.load(pkl_path)\n",
    "    model  = load_model(h5_path)\n",
    "    # Compile to silence \"compiled metrics not built\" warning\n",
    "    n_classes = len(bundle[\"label_encoder\"].classes_)\n",
    "    if n_classes <= 2:\n",
    "        model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return bundle, model\n",
    "\n",
    "def build_df_for_text(single_text: str, bundle: dict) -> pd.DataFrame:\n",
    "    numeric_cols  = bundle.get(\"numeric_cols\", [])\n",
    "    cat_cols      = bundle.get(\"cat_cols\", [])\n",
    "    text_cols     = bundle.get(\"text_cols\", [])\n",
    "    datetime_cols = bundle.get(\"datetime_cols\", [])\n",
    "    all_cols = list(dict.fromkeys(numeric_cols + cat_cols + text_cols + datetime_cols)) or [\"text\"]\n",
    "    df = pd.DataFrame({c: [np.nan] for c in all_cols})\n",
    "    if text_cols:\n",
    "        df[text_cols[0]] = single_text\n",
    "    else:\n",
    "        if \"text\" not in df.columns:\n",
    "            df[\"text\"] = [single_text]\n",
    "        else:\n",
    "            df.loc[0, \"text\"] = single_text\n",
    "    return df\n",
    "\n",
    "def ensure_dense_if_small(X):\n",
    "    if hasattr(X, \"toarray\"):\n",
    "        return X.toarray() if X.shape[1] <= 50000 else X\n",
    "    return X\n",
    "\n",
    "def predict_dataframe(df_in: pd.DataFrame, bundle: dict, model) -> pd.DataFrame:\n",
    "    preprocess     = bundle[\"preprocess\"]\n",
    "    label_encoder  = bundle[\"label_encoder\"]\n",
    "    target_col     = bundle.get(\"target_col\", None)\n",
    "\n",
    "    X = df_in.copy()\n",
    "    if target_col and target_col in X.columns:\n",
    "        X = X.drop(columns=[target_col])\n",
    "\n",
    "    Xt = preprocess.transform(X)\n",
    "    Xt = ensure_dense_if_small(Xt)\n",
    "\n",
    "    probs = model.predict(Xt, verbose=0)\n",
    "    classes = [str(c) for c in label_encoder.classes_]\n",
    "\n",
    "    if probs.ndim == 1 or probs.shape[1] == 1:\n",
    "        pos = probs.ravel()\n",
    "        neg = 1.0 - pos\n",
    "        probs_mat = np.vstack([neg, pos]).T\n",
    "        top_idx = (pos >= 0.5).astype(int)\n",
    "    else:\n",
    "        probs_mat = probs\n",
    "        top_idx = np.argmax(probs_mat, axis=1)\n",
    "\n",
    "    top_label = [classes[i] for i in top_idx]\n",
    "    top_prob  = probs_mat[np.arange(len(probs_mat)), top_idx]\n",
    "\n",
    "    out = pd.DataFrame({\"id\": np.arange(len(df_in)), \"top1_label\": top_label, \"top1_prob\": top_prob})\n",
    "    for j, cls in enumerate(classes):\n",
    "        out[f\"prob_{cls}\"] = probs_mat[:, j]\n",
    "    return out\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Predict mood/emotion labels with MindPal model.\")\n",
    "    parser.add_argument(\"--in\",   dest=\"in_csv\",   type=str, default=None, help=\"Path to input CSV.\")\n",
    "    parser.add_argument(\"--out\",  dest=\"out_csv\",  type=str, default=None, help=\"Output CSV path.\")\n",
    "    parser.add_argument(\"--text\", dest=\"single_text\", type=str, default=None, help=\"Single text to predict.\")\n",
    "    parser.add_argument(\"--print\", dest=\"do_print\", action=\"store_true\", help=\"Print predictions to console.\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    # If no args supplied (e.g., user just ran the file in Jupyter), prompt interactively\n",
    "    if args.single_text is None and args.in_csv is None:\n",
    "        try:\n",
    "            user_text = input(\"Enter a sentence to predict mood (or leave blank to use a CSV): \").strip()\n",
    "        except EOFError:\n",
    "            user_text = \"\"\n",
    "        if user_text:\n",
    "            args.single_text = user_text\n",
    "        else:\n",
    "            csv_guess = input(\"Enter path to input CSV (or press Enter to cancel): \").strip()\n",
    "            if not csv_guess:\n",
    "                raise ValueError(\"Provide either --text 'your sentence' or --in path\\\\to\\\\file.csv\")\n",
    "            args.in_csv = csv_guess\n",
    "\n",
    "    bundle, model = load_bundle_and_model(PKL_PATH, H5_PATH)\n",
    "\n",
    "    if args.single_text is not None:\n",
    "        df_in = build_df_for_text(args.single_text, bundle)\n",
    "    else:\n",
    "        if not os.path.exists(args.in_csv):\n",
    "            raise FileNotFoundError(f\"Input CSV not found: {args.in_csv}\")\n",
    "        df_in = pd.read_csv(args.in_csv)\n",
    "\n",
    "    preds = predict_dataframe(df_in, bundle, model)\n",
    "\n",
    "    out_csv = args.out_csv or os.path.join(OUT_DIR, \"predictions_mood.csv\")\n",
    "    preds.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[SAVE] Predictions -> {out_csv}\")\n",
    "\n",
    "    if args.do_print:\n",
    "        cols_show = [\"id\", \"top1_label\", \"top1_prob\"] + [c for c in preds.columns if c.startswith(\"prob_\")]\n",
    "        print(preds[cols_show].head(20).to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# Jupyter arg-strip + run\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Strip Jupyter's injected args so argparse doesn't choke\n",
    "    if \"ipykernel_launcher\" in sys.argv[0] or any(a == \"-f\" for a in sys.argv):\n",
    "        sys.argv = [sys.argv[0]] + sys.argv[1:][:0]  # keep just script name (no extra args)\n",
    "        # You can also prefill defaults for quick tests, e.g.:\n",
    "        # sys.argv += [\"--text\", \"I feel focused and calm.\", \"--print\"]\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7f06f2-d07c-49dc-b0f4-3a5e5c77f092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
